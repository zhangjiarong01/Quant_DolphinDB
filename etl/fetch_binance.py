import os
import json
import pandas as pd
import requests
import zipfile
import io
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import dolphindb as ddb

# === é…ç½® ===
ddb_host = "192.168.100.43"
ddb_port = 7739
ddb_user = "admin"
ddb_pass = "123456"
db_path = "dfs://crypto_kline_1h"
table_name = "kline_1h"
tmp_dir = "tmp"
parquet_dir = "parquet_cache"
log_file = "import_log.json"

# åˆå§‹åŒ– DolphinDB è¿æ¥
s = ddb.session()
s.connect(ddb_host, ddb_port, ddb_user, ddb_pass)
appender = ddb.TableAppender(db_path, table_name, s)

# æœ¬åœ°ç›®å½•å‡†å¤‡
os.makedirs(tmp_dir, exist_ok=True)
os.makedirs(parquet_dir, exist_ok=True)

# å¯¼å…¥æ—¥å¿—æœºåˆ¶
try:
    with open(log_file, "r") as f:
        import_log = json.load(f)
except (FileNotFoundError, json.JSONDecodeError):
    import_log = {}

def already_imported(symbol, year, month):
    return f"{symbol}-{year}-{month:02}" in import_log

def mark_imported(symbol, year, month):
    import_log[f"{symbol}-{year}-{month:02}"] = True
    with open(log_file, "w") as f:
        json.dump(import_log, f, indent=2)

# ä¸‹è½½å¹¶è§£å‹
def download_and_extract(symbol, interval, year, month):
    url = f"https://data.binance.vision/data/spot/monthly/klines/{symbol}/{interval}/{symbol}-{interval}-{year}-{month:02}.zip"
    try:
        print(f"ğŸ“¥ ä¸‹è½½ï¼š{url}")
        r = requests.get(url, timeout=15)
        r.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯200ï¼Œä¼šæŠ›å‡ºHTTPErrorå¼‚å¸¸
        
        with zipfile.ZipFile(io.BytesIO(r.content)) as z:
            z.extractall(tmp_dir)
            filename = z.namelist()[0]
            return os.path.join(tmp_dir, filename)
    except Exception as e:
        with open("fail_log.txt", "a") as f:
            f.write(f"{datetime.now().isoformat()} | {symbol}-{year}-{month:02} | ERROR: {str(e)}\n")
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼š{url} - {e}")
        return None

# è½¬æ¢ä¸º Parquet
def load_csv_to_parquet(csv_path, symbol):
    try:
        df = pd.read_csv(csv_path, header=None)
        df.columns = [
            "open_time", "open", "high", "low", "close", "volume",
            "close_time", "quote_volume", "n_trades",
            "taker_base", "taker_quote", "ignore"
        ]
        df["symbol"] = symbol
        df["open_time"] = pd.to_datetime(df["open_time"], unit='ms')
        df["close_time"] = pd.to_datetime(df["close_time"], unit='ms')
        df["update_time"] = datetime.now()
        df.drop(columns=["ignore"], inplace=True)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(parquet_dir, exist_ok=True)
        
        # ä½¿ç”¨æ›´å®‰å…¨çš„æ–‡ä»¶å
        first_date = df['open_time'].iloc[0].strftime('%Y%m') if not df.empty else 'nodata'
        parquet_path = os.path.join(parquet_dir, f"{symbol}_{first_date}.parquet")
        df.to_parquet(parquet_path, index=False)
        return parquet_path
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥ï¼š{csv_path} - {e}")
        return None

# ä¸»å¤„ç†å‡½æ•°
def process(symbol, interval, year, month):
    if already_imported(symbol, year, month):
        print(f"â© è·³è¿‡ {symbol}-{year}-{month:02}ï¼ˆæ—¥å¿—å·²è®°å½•ï¼‰")
        return
    
    csv_path = download_and_extract(symbol, interval, year, month)
    if not csv_path:
        return
    
    parquet_path = load_csv_to_parquet(csv_path, symbol)
    if not parquet_path:
        return
    
    try:
        df = pd.read_parquet(parquet_path)
        appender.append(df)
        print(f"âœ… å†™å…¥æˆåŠŸï¼š{symbol}-{year}-{month:02}")
        mark_imported(symbol, year, month)
    except Exception as e:
        print(f"âŒ å†™å…¥DolphinDBå¤±è´¥ï¼š{symbol}-{year}-{month:02} - {e}")
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if csv_path and os.path.exists(csv_path):
            try:
                os.remove(csv_path)
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼š{csv_path} - {e}")
        
        if parquet_path and os.path.exists(parquet_path):
            try:
                os.remove(parquet_path)
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼š{parquet_path} - {e}")

# å¹¶è¡Œå¤„ç†å…¥å£
def run_batch(symbols, years, months, max_workers=5):
    tasks = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for symbol in symbols:
            for year in years:
                for month in months:
                    tasks.append(executor.submit(process, symbol, "1h", year, month))
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for task in tasks:
            try:
                task.result()
            except Exception as e:
                print(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

# ======= å¤šå¸ç§æ‰¹é‡å¯¼å…¥å…¥å£ =======
if __name__ == "__main__":
    symbols = [ "BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT", "XRPUSDT", "DOGEUSDT", "ADAUSDT", "AVAXUSDT", "DOTUSDT", "MATICUSDT",
    "TRXUSDT", "SHIBUSDT", "LINKUSDT", "LTCUSDT", "UNIUSDT", "ATOMUSDT", "BCHUSDT", "ETCUSDT", "XLMUSDT", "FILUSDT",
    "HBARUSDT", "ICPUSDT", "APTUSDT", "ARBUSDT", "NEARUSDT", "OPUSDT", "VETUSDT", "GRTUSDT", "MKRUSDT", "SANDUSDT",
    "EOSUSDT", "AAVEUSDT", "RUNEUSDT", "THETAUSDT", "QNTUSDT", "FTMUSDT", "XTZUSDT", "FLOWUSDT", "CHZUSDT", "EGLDUSDT",
    "AXSUSDT", "SNXUSDT", "KAVAUSDT", "TWTUSDT", "IMXUSDT", "CRVUSDT", "GMXUSDT", "CFXUSDT", "DYDXUSDT", "LDOUSDT",
    "ZECUSDT", "1INCHUSDT", "ALGOUSDT", "ROSEUSDT", "COMPUSDT", "YFIUSDT", "STXUSDT", "WOOUSDT", "ENSUSDT", "IOTAUSDT",
    "KSMUSDT", "CELOUSDT", "GMTUSDT", "OCEANUSDT", "BALUSDT", "ZILUSDT", "ANKRUSDT", "LINAUSDT", "SKLUSDT", "XEMUSDT",
    "BLZUSDT", "SXPUSDT", "ARPAUSDT", "BANDUSDT", "COTIUSDT", "RLCUSDT", "DGBUSDT", "STMXUSDT", "MTLUSDT", "NKNUSDT",
    "BELUSDT", "HOOKUSDT", "HIGHUSDT", "JOEUSDT", "TUSDT", "TRUUSDT", "PEOPLEUSDT", "MDTUSDT", "FETUSDT", "CKBUSDT",
    "DODOUSDT", "BNTUSDT", "OMGUSDT", "VTHOUSDT", "WNXMUSDT", "POWRUSDT", "REIUSDT", "FLMUSDT", "XNOUSDT", "RSRUSDT"]
  # ä¿®æ­£äº†è¿™é‡Œçš„é‡å¤èµ‹å€¼
    run_batch(symbols=symbols, years=range(2021, 2024), months=range(1, 13), max_workers=8)
    